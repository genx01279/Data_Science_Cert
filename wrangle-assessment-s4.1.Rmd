---
title: "Wrangling-4.1. Assessment"
author: "Lauren"
date: "`r Sys.Date()`"
output: html_document
---
**Assessment Part 1: Dates, Times, and Text Mining**

This assessment reviews several concepts about dates, times, and text mining. In part 1 on this page, you will practice extracting and manipulating dates in real datasets. In part 2 on the next page, you will walk through a sentiment analysis of a novel using steps covered in the previous section.

Use the following libraries and options for coding questions:
```{r}
library(dslabs)
library(lubridate)
library(dplyr)
options(digits = 3)    # 3 significant digits
```

IMPORTANT: Some of these exercises use dslabs datasets that were added in a July 2019 update. Make sure your package is up to date with the command install.packages("dslabs").

##Question 1
Which of the following is the standard ISO 8601 format for dates?
YYYY-MM-DD

##Question 2
Which of the following commands could convert this string into the correct date format?
dates <- c("09-01-02", "01-12-07", "02-03-04")

It is impossible to know which format is correct without additional information.

##Question 3
Load the brexit_polls data frame from dslabs:
```{r}
data(brexit_polls)
```

How many polls had a start date (startdate) in April (month number 4)?
```{r}
brexit_polls %>% filter(month(startdate, label = FALSE)== 4) %>% nrow()
```

Use the round_date() function on the enddate column with the argument unit="week". How many polls ended the week of 2016-06-12?
```{r}
brexit_polls %>% filter(round_date(enddate, unit = "week")=="2016-06-12") %>% nrow()
```

##Question 4
Use the weekdays() function from lubridate to determine the weekday on which each poll ended (enddate).

On which weekday did the greatest number of polls end?
```{r}
brexit_polls %>% count(weekdays(enddate)) %>% arrange(desc(n))
```

##Question 5
Load the movielens data frame from dslabs.
```{r}
data(movielens)
```
This data frame contains a set of about 100,000 movie reviews. The timestamp column contains the review date as the number of seconds since 1970-01-01 (epoch time).

Convert the timestamp column to dates using the lubridate as_datetime() function.

Which year had the most movie reviews?
```{r}
movielens <- movielens %>% 
  mutate(review_timestamp = as_datetime(timestamp)) 
movielens %>% 
  count(year(review_timestamp)) %>% 
  arrange(desc(n))
```

Which hour of the day had the most movie reviews?
```{r}
movielens %>% 
  count(hour(review_timestamp)) %>% 
  arrange(desc(n))
```

**Assessment Part 2: Dates, Times, and Text Mining**
In this part of the assessment, you will walk through a basic text mining and sentiment analysis task.

Project Gutenberg is a digital archive of public domain books. The R package gutenbergr facilitates the importation of these texts into R. We will combine this with the tidyverse and tidytext libraries to practice text mining.
Use these libraries and options:
```{r}
library(tidyverse)
library(gutenbergr)
library(tidytext)
options(digits = 3)
```

You can see the books and documents available in gutenbergr like this:
```{r}
gutenberg_metadata
```

##Question 6
Use str_detect() to find the ID of the novel Pride and Prejudice.

How many different ID numbers are returned?

```{r}
pp <- gutenberg_metadata %>% 
  filter(str_detect(title, pattern="Pride and Prejudice", negate=FALSE)) 
pp %>% distinct(gutenberg_id)
```

## Question 7

Notice that there are several versions of the book. The gutenberg_works() function filters this table to remove replicates and include only English language works. Use this function to find the ID for Pride and Prejudice.

What is the correct ID number?

```{r}
gutenberg_works(title=="Pride and Prejudice",languages = "en")$gutenberg_id
```

##Question 8
Use the gutenberg_download() function to download the text for Pride and Prejudice. Use the tidytext package to create a tidy table with all the words in the text. Save this object as words.

How many words are present in the book?
```{r}
words <- gutenberg_download(gutenberg_id = 1342)
words <- words %>% unnest_tokens(word, text)
nrow(words)
```

##Question 9
Remove stop words from the words object. Recall that stop words are defined in the stop_words data frame from the tidytext package.

How many words remain?
```{r}
stop_words
words <- words %>% anti_join(stop_words)
nrow(words)
```
##Question 10
After removing stop words, detect and then filter out any token that contains a digit from words.

How many words remain?
```{r}
pattern <- "[0-9]"
words <- words %>% filter(!str_detect(word, pattern))
nrow(words)
```

##Question 11
Analyze the most frequent words in the novel after removing stop words and tokens with digits.

How many words appear more than 100 times in the book?
```{r}
words %>% count(word) %>% filter(n>100) %>% arrange(desc(n))
```

##Question 12
Define the afinn lexicon:
```{r}
afinn <- get_sentiments("afinn")
```
Note that this command will trigger a question in the R Console asking if you want to download the AFINN lexicon. Press 1 to select "Yes" (if using RStudio, enter this in the Console tab).

Use this afinn lexicon to assign sentiment values to words. Keep only words that are present in both words and the afinn lexicon. Save this data frame as afinn_sentiments.

How many elements of words have sentiments in the afinn lexicon?
```{r}
afinn_sentiments <- words %>% inner_join(afinn)
nrow(afinn_sentiments)
```

What proportion of words in afinn_sentiments have a positive value?
```{r}
positive <- afinn_sentiments %>% filter(value >0) %>% arrange(desc(value))
positive
nrow(positive)/nrow(afinn_sentiments)
```

How many elements of afinn_sentiments have a value of 4?
```{r}
positive %>% filter(value ==4) %>% nrow()
```

